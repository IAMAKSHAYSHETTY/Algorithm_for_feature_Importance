# -*- coding: utf-8 -*-
"""Business.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1YEGRzvhUx4fEkGfb5Wlm5ItQ9ciuLKBw
"""

import pandas as pd
from difflib import SequenceMatcher
from fuzzywuzzy import fuzz
import numpy as np
from matplotlib import pyplot as plt

import warnings
warnings.filterwarnings("ignore")

main = pd.read_csv("/content/Main.csv")
main.fillna('', inplace=True)
main.drop_duplicates()
main = main.head(10000)
main['Indigenous Identity'] = ""
main['Ethnicity'] = ""
main['Place of Birth'] = ""

duplicates = pd.read_csv("/content/Duplicates.csv")
duplicates.fillna('', inplace=True)
duplicates.drop_duplicates()
duplicates = duplicates.head(10000)

col_to_drop = ['ID']
duplicates.drop(col_to_drop, axis=1, inplace=True)
main.drop(col_to_drop, axis=1, inplace=True)

target_df = pd.read_csv("/content/Main-1.csv")
target = pd.DataFrame(target_df['Quality'])
target = target.head(10000)

Weights_df = pd.DataFrame()

cols = ['Last Name', 'First Name', 'Mother\'s Birth Last Name', 'Date of Birth', 'Language (Mother Tongue)', 'Citizenship', 'Religion', 'Indigenous Identity', 'Ethnicity', 'Place of Birth']
main = main[cols]
duplicates = duplicates[cols]

#Training
#Finding Similarity Score of Individual Element
for i in range(main.shape[0]):
  dup_data = duplicates.iloc[[i]]
  main_data = main.iloc[[i]]
  #Original Score
  OriginalTarget = target.iloc[i][0]
  print(OriginalTarget)
  sscore = []
  k=0
  for j in range(main.shape[1]):
      i1 = dup_data.iloc[0][j]
      print(i1)
      print("/n")
      i2 = main_data.iloc[0][k]
      print(i2)
      if(i1!="" or i2!=""):
        ss = fuzz.ratio(i1, i2)
        sscore.append(ss)
        print(f"Similarity score: {fuzz.ratio(i1, i2)}")
      else:
        ss = 0
        sscore.append(ss)
        print(f"Similarity score: 0")
      print("-----")
      k = k+1

  #Scores of variables
  sscore = np.array(sscore)
  print(sscore)

  #Current Score
  currentScore = sscore.sum()/sscore.shape[0]
  print(currentScore)

  #Scores of Variables after noramlised out of 100
  SS_AfterNorm = sscore/10
  print(SS_AfterNorm)
  print(SS_AfterNorm.sum())

  #Finding the total difference
  diff = OriginalTarget - currentScore

  #Impact of varibale to the current Score
  propotions = []
  for n in sscore:
    if n!=0:
      p = n/sscore.sum()
      propotions.append(p)
    else:
      p=0
      propotions.append(p)


  propotions = np.array(propotions)

  #Weights to be added(to the SS_AfterNorm) given to individual variables
  weights = propotions * diff
  weights1 = pd.DataFrame(weights)
  Weights_df = pd.concat([Weights_df, weights1], axis=1)

  #New Scores of the Variables
  New_Score = SS_AfterNorm + weights
  print(New_Score)
  print(New_Score.sum())


  print(Weights_df)

Weights_df['Average Weight'] = Weights_df.sum(axis=1)/10
Weights_df['Variables'] = cols
Weights_df

Weights_df.sort_values('Average Weight', ascending=False)

max = Weights_df['Average Weight'].max()
min = Weights_df['Average Weight'].min()

Weights_df['normalized_value'] = (Weights_df['Average Weight'] - min) / (max - min) * (1 - 0) + 0

Weights_df.sort_values('normalized_value', ascending=False)



Weights_data = Weights_df[["Variables", "Average Weight", "normalized_value"]]

Weights_data.sort_values('normalized_value', ascending=False)

Weights_data

Weights_data.plot.barh('Variables', 'normalized_value')

#Weight Values for classification
weight_values = np.array(Weights_data['Average Weight'])

#Validate

valid_main_df = pd.read_csv('/content/Validation - Main.csv')
valid_main_df.fillna('', inplace=True)
valid_main_df.drop_duplicates()
valid_main_df['Indigenous Identity'] = ""
valid_main_df['Ethnicity'] = ""
valid_main_df['Place of Birth'] = ""
match_df = valid_main_df['Match']
main_ID = valid_main_df['ID'][1:]


valid_dup_df = pd.read_csv('/content/Validation - Dup.csv')
valid_dup_df.fillna('', inplace=True)
valid_dup_df.drop_duplicates()

valid_main_df = valid_main_df[cols]
valid_dup_df = valid_dup_df[cols]

valid_main_df = valid_main_df.head(2)
valid_dup_df = valid_dup_df.head(2)

valid_main_df

validation_df = pd.DataFrame()
for x in range(valid_dup_df.shape[0]):
  vd = valid_dup_df.iloc[[x]]
  validation_df = pd.concat([validation_df, pd.DataFrame(vd)])
  validation_main_df = pd.DataFrame()
  validScore = []
  for y in range(valid_main_df.shape[0]):
    vm = valid_main_df.iloc[[y]]
    id = np.array(main_ID[y+1])
    validation_main_df = pd.concat([validation_main_df, pd.DataFrame([id])])
    for z in range(valid_main_df.shape[1]):
      i3 = vd.iloc[0][z]
      i4 = vm.iloc[0][z]
      w = weight_values[z]
      if(i3!="" or i4!=""):
          vs = fuzz.ratio(i3, i4)
          vs = vs+w
          validScore = np.append(validScore,vs)
          print(f"Similarity score: {fuzz.ratio(i3, i4)}")
      else:
          vs = 0
          vs = vs+w
          validScore = np.append(validScore,vs)
          print(f"Similarity score: 0")
    validScore = np.array(validScore)
    validScore = validScore/10
    validsum = validScore.sum()
    validsum = np.array([validsum])
    print(validsum)
    validsum = pd.DataFrame(validsum, columns=['Score'])
    validation_main_df = pd.concat([validation_main_df, validsum], ignore_index=True)
    print(validation_main_df.fillna(method='ffill')
  #validation_df = pd.concat([validation_df, validation_main_df], axis =1)

